# Whisper App - Project Guide

## Project Overview

Web application for real-time voice transcription using OpenAI Whisper running locally. Built with React + Vite frontend and Python Flask backend.

## Tech Stack

### Frontend
- **React 18** - UI framework
- **Vite** - Build tool and dev server
- **Web Audio API** - Audio recording and visualization
- **CSS3** - Modern gradients, animations, glassmorphism

### Backend
- **Python 3.9+** - Runtime
- **Flask 3.0** - Web framework
- **OpenAI Whisper** - Speech-to-text model
- **Flask-CORS** - Cross-origin requests

## Project Structure

```
whisper-app/
├── src/                    # Frontend React source
│   ├── App.jsx            # Main component with recording logic
│   ├── App.css            # Styles with gradients and animations
│   ├── index.css          # Global styles
│   └── main.jsx           # React entry point
│
├── api-python/            # Backend Python API
│   ├── server.py          # Flask server with Whisper integration
│   └── requirements.txt   # Python dependencies
│
├── server/                # Legacy Node.js backend (not used)
│
├── start-python.sh        # Start script (backend + frontend)
└── package.json           # Frontend dependencies
```

## Setup & Installation

### First Time Setup

```bash
# Install frontend dependencies
npm install

# Install backend dependencies
cd api-python
pip3 install -r requirements.txt
```

### Running the Application

```bash
# Start both frontend and backend
./start-python.sh

# Or manually:
# Terminal 1 - Backend
cd api-python
python3 server.py

# Terminal 2 - Frontend
npm run dev
```

**Ports:**
- Frontend: http://localhost:5173
- Backend: http://localhost:5001

## Development Workflow

### Making Changes

1. **Frontend changes**: Edit files in `src/`, hot-reload is automatic
2. **Backend changes**: Restart `python3 server.py` after changes
3. **Styles**: Edit `App.css`, changes reflect immediately

### Testing

1. Open http://localhost:5173
2. Allow microphone access
3. Click "Grabar" → speak → click "Detener"
4. Verify transcription appears

### Common Issues

**Port 5001 already in use:**
```bash
lsof -i :5001
kill <PID>
```

**Model loading slow:**
- First run downloads ~1.5GB model
- Subsequent runs load from cache (~10-20 seconds)

**Microphone not working:**
- Check browser permissions (Settings → Privacy → Microphone)
- macOS: System Preferences → Security & Privacy → Microphone

## Features

### Current
- ✅ Audio recording with pause/resume
- ✅ Real-time waveform visualization
- ✅ Spanish transcription (model: medium)
- ✅ Copy to clipboard
- ✅ Recording timer

### Architecture Notes

**Frontend:**
- Uses `MediaRecorder` API for audio capture
- Web Audio API for waveform visualization
- Sends audio as WAV blob to backend on stop

**Backend:**
- Loads Whisper model once on startup
- Receives audio files via multipart/form-data
- Returns JSON with transcription
- Cleans up temp files automatically

## Commit Guidelines

Follow conventional commit format:

```
<type>: <description>

[optional body]
```

**Types:** feat, fix, docs, refactor, test, chore, style, perf

**Important:** Do NOT include any AI tool mentions in commit messages.

### Good Examples
```
feat: add pause/resume functionality to audio recorder
fix: correct audio level visualization timing
refactor: extract transcription logic to separate function
```

### Bad Examples
```
feat: add feature (Generated by AI)  ❌
update code  ❌
```

## API Endpoints

### GET /health
Check server status

**Response:**
```json
{
  "status": "ok",
  "message": "Servidor Whisper Python funcionando"
}
```

### POST /transcribe
Transcribe audio file

**Request:**
- Content-Type: multipart/form-data
- Field: `audio` (WAV file)

**Response:**
```json
{
  "success": true,
  "transcription": "texto transcrito..."
}
```

## Performance

- **Model Load Time**: 10-20 seconds (first time: ~2 minutes for download)
- **Transcription Speed**: ~3-5 seconds for 10 seconds of audio
- **Supported Audio**: Any format FFmpeg can read (WAV, MP3, M4A, etc.)

## Environment

- **Node.js**: v19.9.0 (or compatible)
- **Python**: 3.9.6+
- **OS**: macOS (Apple Silicon optimized)

## Dependencies

### Python
- flask==3.0.0
- flask-cors==4.0.0
- openai-whisper==20231117

### Node.js
- react: ^18.3.1
- vite: ^6.0.5

## Future Improvements

Ideas for enhancement:
- [ ] Support for different Whisper models (small, large)
- [ ] Language selection in UI
- [ ] Export transcription to file
- [ ] Dark mode
- [ ] Audio file upload (not just recording)
- [ ] Transcription history

## Notes

- Whisper model runs locally (no API calls, data stays private)
- GPU acceleration automatic on Apple Silicon
- First transcription may be slow (model loading)
- Spanish language optimized with `-l es` flag
